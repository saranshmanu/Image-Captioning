{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgXoR8glOfkq"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ot62cqjmDKPI",
    "outputId": "9149d890-a050-422a-a086-ac1d8fa65a70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_df-UVIOsK3"
   },
   "source": [
    "### Getting the files from the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "colab_type": "code",
    "id": "4QFdwmfyXFOX",
    "outputId": "c3296661-8e8d-437b-d9f9-2bbfcc3535c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fdg-EhNgZZkz",
    "outputId": "6b8ab061-5266-4e57-e2e8-0c9437fd5277"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dataset_url = 'dataset/'\n",
    "os.listdir(base_dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4eunhUWOwh_"
   },
   "source": [
    "### Extracting the comments and the image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeccdr0iaNhb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_dataset_url + \"results.csv\", sep='|', header=None)\n",
    "df = df[df[2].notnull()]\n",
    "df = df[df[0] != 'image_name']\n",
    "comments = df[2].tolist()\n",
    "image_names = df[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeR-KTg9NcGe"
   },
   "source": [
    "### Finding the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9L-SR-pbALO"
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for sentence in comments:\n",
    "    words_flag = sentence.split(' ')\n",
    "    words += words_flag\n",
    "words = [i.lower() for i in words if i.isalpha()]\n",
    "unique_words = []\n",
    "for i in words:\n",
    "    if i not in unique_words:\n",
    "        unique_words.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFDZQZttOb8R"
   },
   "source": [
    "### Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-RS00vRSKW0"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "\n",
    "number_of_words = 20000\n",
    "tokenizer = Tokenizer(number_of_words)\n",
    "tokenizer.fit_on_texts(comments)\n",
    "words_dictionary = tokenizer.word_index\n",
    "vocab_size = len(words_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxg0adODTtId"
   },
   "outputs": [],
   "source": [
    "x_text = tokenizer.texts_to_sequences(comments)\n",
    "x_text = pad_sequences(x_text)\n",
    "y_text = np_utils.to_categorical(x_text, num_classes = vocab_size + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hbYn6CwIcw9M"
   },
   "outputs": [],
   "source": [
    "x_text = np.array(x_text)\n",
    "y_text = np.array(y_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vYQ71HThLi1q",
    "outputId": "f2c59040-237d-4e81-d9a3-41acac6e376e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image_name'"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(\"x_text.npy\", x_text)\n",
    "np.save(\"y_text.npy\", y_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iGDt5IytMU1m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image Captioning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
